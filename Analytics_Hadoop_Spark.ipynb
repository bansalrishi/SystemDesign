{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "- Open Source\n",
    "- Distributed store and computing\n",
    "- Can scale to petabytes of data\n",
    "- Consists of\n",
    "    - Hadoop Distributed File System (HDFS)\n",
    "    - MapReduce\n",
    "    \n",
    "Source: LinkedIn: big-data-analytics-with-hadoop-and-apache-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical on \n",
    "- Ambari sandbox\n",
    "- Zeppelin notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Distributed File System (HDFS)\n",
    "- good and cheap option to store large amount of data\n",
    "- provides scaling, security and cost benefit\n",
    "- suitable for enterprises with in-house data centers\n",
    "- Cloud Alternatives - Amazon S3, Oracle OSS, Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "- Scales Horizontally\n",
    "- Very Slow as it uses disk storage for internediate caching instead of memory\n",
    "- Faster Alternatives - Apache Spark, Apache Flink\n",
    "- These alternatives also has growing list of supporting libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "- Open Source\n",
    "- Large scale distributed data processing engine\n",
    "- Uses memory to speed up computations\n",
    "- Batch, streaming, ML and graph capabilities\n",
    "- Support Scala, Java, Python and R \n",
    "- Most popular big data platform today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop and Spark\n",
    "- Spark is well integrated with Hadoop\n",
    "- Spark can access and process data HDFS using \n",
    "    - parallel nodes\n",
    "    - read optimization to use less memory and I/O\n",
    "    - use HDFS for intermediate Data Caching\n",
    "    - Yarn provides single cluster management for both HDFS and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Data Modelling for Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hadoop Storage Formats\n",
    "- Raw Texts (blobs)\n",
    "- Structured text files (csv, xml, json)\n",
    "- Sequence Files\n",
    "- Avro\n",
    "- ORC\n",
    "- Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Files:  \n",
    "- Simple to read/write\n",
    "- Low performance - no parallel operations\n",
    "- More Storage\n",
    "- No schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro:  \n",
    "- Language neutral data serialization\n",
    "- Row format\n",
    "- Self-describing schema support\n",
    "- Compressible\n",
    "- Splittable\n",
    "- Ideal for multi-language support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet:  \n",
    "- Columnar Format\n",
    "- Read-only selected columns (saves I/O)\n",
    "- Schema Support\n",
    "- Compressible (column level) and Splittable\n",
    "- Supports nested Data Structures\n",
    "- Ideal for analytics applications  \n",
    "\n",
    "Parquet for Analytics:  \n",
    "- provides overall better performance and flexibility for analytics applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hadoop Compression Options\n",
    "- Snappy\n",
    "    - Compression codec developed by google\n",
    "    - Moderate Compression\n",
    "    - Excellent read/write performance\n",
    "    - Compresses entire file\n",
    "    - Not splittable so dont support parallel operations\n",
    "- LZO\n",
    "    - Moderate compression\n",
    "    - Excellent processing performance\n",
    "    - Splittable - Support parallel processing\n",
    "    - requires separate license\n",
    "- GZIP\n",
    "    - Very good compression\n",
    "    - Moderate processing performance\n",
    "    - Not Splittable\n",
    "    - Ideal for container type applications\n",
    "- bzip2\n",
    "    - Excellent compression\n",
    "    - Slower processing performance \n",
    "    - Splittable\n",
    "    - Ideal for archival type applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Partitioning\n",
    "- HDFS does not have the concept of indexes\n",
    "- Even for reading one row, the entire file should be read\n",
    "- Partitioning provides a way to read only a subset of data\n",
    "- Multiple attributes can be used for hierarchical partitioning\n",
    "- Split data into directories based on individual values of attributes\n",
    "\n",
    "<img src=\"Image/partitioning.JPG\" width=\"600\" />\n",
    "\n",
    "- choose attributes with a limited set of values and those that are most used in SELECT filters\n",
    "- otherwise many sub directories will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bucketing\n",
    "- Partitioning is optimal when an attribute is having small set of unique values\n",
    "- What if we need to partition based on a key having large no of values\n",
    "- Similar to Partioning but instead of value it uses a Hash function to convert value to a Hash key\n",
    "- Controls number of unique directories created\n",
    "- Even disribution\n",
    "- Choose attributes with large number of unique values and those that are most used in SELECT filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. HDFS Schema Designing and Storage Best Practices\n",
    "- understand the data whether its read intensive or write intensive or both\n",
    "- determine what needs optimization and what can be compromised (reduce storage req or compromise storage for better read/write performance)\n",
    "- choose options carefully as they cant be changed easily\n",
    "- run tests on actual data to understand performance and storage caracteristics\n",
    "- choose partitioning and bucketing keys wisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exercises - Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Reading Files into Spark\n",
    "Data can be read into Apache Spark data frames from a variety of data sources.\n",
    "\n",
    "examples :\n",
    "\n",
    "- A flat file on a local disk\n",
    "- A file from HDFS\n",
    "- A Kafka Topic\n",
    "- In this example, we will read a CSV file in a HDFS folder into a Spark Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the raw CSV file int a Spark DataFrame\n",
    "#    Use inferSchema to infer the schema automatically from the CSV file\n",
    "val rawSalesData = spark\n",
    "                .read\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"/user/raj_ops/raw_data/sales_orders.csv\");\n",
    "\n",
    "#Print the schema for verification\n",
    "rawSalesData.printSchema();\n",
    "\n",
    "#Print the first 5 records for verification\n",
    "rawSalesData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.Writing to HDFS\n",
    "Write the rawSalesData Data Frame into HDFS as a Parquet file. Use Parquet as the format since it enables splitting and filtering. Use GZIP as the compression codec.\n",
    "\n",
    "On completion, verify if the files are correctly through HDFS command line or Ambari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to Sales Data to HDFS for future processing\n",
    "\n",
    "rawSalesData.write\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"compression\", \"gzip\")\n",
    "            .save(\"/user/raj_ops/raw_parquet\");\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Writing to HDFS with Partitioning\n",
    "Write a partitioned Parquet file in HDFS. Partition will be done by Product. This will create one directory per unique product available in the raw CSV. Verify through HDFS command line or Ambari\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawSalesData.write\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"compression\", \"gzip\")\n",
    "            .partitionBy(\"Product\")\n",
    "            .save(\"/user/raj_ops/partitioned_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. Writing to Hive with Bucketing\n",
    "Create a Bucketed Hive table for orders. Bucketing will be done by Product. It will create 3 buckets based on the hash generated by Product. Hive tables can be queried through SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Hive Table for sales data with 2 buckets.\n",
    "rawSalesData.write\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .bucketBy(3, \"Product\")\n",
    "            .saveAsTable(\"product_bucket_table\")\n",
    "            \n",
    "#Data goes in here.\n",
    "println(\"Hive Data Stored in : \" + sc.getConf.get(\"spark.sql.warehouse.dir\") + \"\\n\")\n",
    "            \n",
    "#Read through SQL\n",
    "sql(\"SELECT * FROM product_bucket_table where Product='Mouse'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion with Spark Best Practices\n",
    "- Enable parallelism for max write performance \n",
    "    - use splitable file format like parquet\n",
    "    - use partitions or buckets\n",
    "- Use APPEND for incremental data ingestion\n",
    "- External data reads - use resources that provide parallelism\n",
    "    - e.g: JDBC, Kafka\n",
    "    - Break down large files into smaller files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Spark Works - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark programs run in driver node which use Spark cluster to execute them\n",
    "- Spark cluster have multiple Executer node which execute programs in parallel\n",
    "\n",
    "<img src=\"Image/spark_arc.JPG\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Execution Plan\n",
    "- Lazy Execution - only an action triggers execution\n",
    "- Spark optimizer comes with a Physical Plan\n",
    "- Physical plan optimizes for:\n",
    "    - Reduced I/O\n",
    "    - Reduced Shuffling\n",
    "    - Reduced Memory Usage\n",
    "- Spark executers can read and write directly from external sources when they support parallel I/O and reduce memory req at the driver\n",
    "    - e.g: HDFS, Kafka, JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exercises - Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Read Parquet Files into Spark\n",
    "Read a non-partitioned Parquet file into Spark. Measure the time taken. Also look at the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "val salesParquet = spark.read\n",
    "                        .parquet(\"/user/raj_ops/raw_parquet\")\n",
    "\n",
    "#Display the results and time the operation                     \n",
    "spark.time(salesParquet.show(5))\n",
    "\n",
    "#Show the execution Plan\n",
    "println(\"\\n-------------------------------EXPLAIN------------------------------------\")\n",
    "salesParquet.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Physical Plan ==  \n",
    "*(1) FileScan parquet [ID#140,Customer#141,Product#142,Date#143,Quantity#144,Rate#145,Tags#146] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Read Partitioned Data into Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the partitions. Use basePath to have partition key as part of data\n",
    "val salesPartitioned = spark.read\n",
    "                            .option(\"basePath\", \"/user/raj_ops/partitioned_parquet/\")\n",
    "                            .parquet(\"/user/raj_ops/partitioned_parquet/*\")\n",
    "                            \n",
    "#Display the results and time the operation                     \n",
    "spark.time(salesPartitioned.show(5))\n",
    "\n",
    "#Show the execution Plan\n",
    "println(\"\\n-------------------------------EXPLAIN------------------------------------\")\n",
    "salesPartitioned.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Physical Plan ==  \n",
    "*(1) FileScan parquet [ID#178,Customer#179,Date#180,Quantity#181,Rate#182,Tags#183,Product#184] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a specific partition only\n",
    "val salesHeadset = spark.read\n",
    "                            .parquet(\"/user/raj_ops/partitioned_parquet/Product=Headset\")\n",
    "\n",
    "#Display the results and time the operation                     \n",
    "spark.time(salesHeadset.show(5))\n",
    "\n",
    "#Show the execution Plan\n",
    "println(\"\\n-------------------------------EXPLAIN------------------------------------\")\n",
    "salesHeadset.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Physical Plan == Partition Count is not printed as only one sub directory is read     \n",
    "*(1) FileScan parquet [ID#216,Customer#217,Date#218,Quantity#219,Rate#220,Tags#221] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Read Bucketed Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from Hive\n",
    "val salesBucketed = sql(\"SELECT * FROM product_bucket_table\")\n",
    "\n",
    "#Display the results and time the operation                     \n",
    "spark.time(salesBucketed.show(5))\n",
    "\n",
    "#Show the execution Plan\n",
    "println(\"\\n-------------------------------EXPLAIN------------------------------------\")\n",
    "salesBucketed.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Physical Plan ==Its not different reading file from HDFS, Spark DataFrame, Datasets, SQL and RDDs   \n",
    "*(1) FileScan parquet default.product_bucket_table[ID#103,Customer#104,Product#105,Date#106,Quantity#107,Rate#108,Tags#109] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/apps/spark/warehouse/product_bucket_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction with Spark Best Practices\n",
    "- reduce data read into memory by:\n",
    "    - using filters based on partition key\n",
    "    - reading only required columns\n",
    "- use data sources and file formats that support parallelism\n",
    "- keep number of partitions >= (No. of executors * No. of cores per executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exercises - Optimizing Spark Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Pushing down Projections\n",
    "When downstream queries/processing only looks for a subset of columns, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Projection Push down. While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read sales data from partitioned parquet file\n",
    "val salesData = spark.read\n",
    "                .option(\"basePath\", \"/user/raj_ops/partitioned_parquet/\")\n",
    "                .parquet(\"/user/raj_ops/partitioned_parquet/*\")\n",
    "                \n",
    "#Projection gets pushed down to the file scan\n",
    "println(\"-------------------------------EXPLAIN------------------------------------\")\n",
    "salesData.select(\"Product\",\"Quantity\").explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- Here we are ready whole file in DataFrame but selecting only two columns\n",
    "- Spark is smart enough to copy only two columns into memory  \n",
    "- So, for troubleshooting its not wise to select all columns, we can select one column for faster execution   \n",
    "== Physical Plan ==     \n",
    "*(1) Project [Product#303, Quantity#300]  \n",
    "+- *(1) FileScan parquet [Quantity#300,Product#303] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Pushing down Filters\n",
    "When downstream queries/processing only looks for a subset of subset, Spark optimizer is smart enough to identify them and only read those columns into the in-memory data frame. This saves on I/O and memory. This is called Filter Push down. This works for both partition columns (Product) and non-partition columns (Customer). While building data pipelines, it helps to be aware of how Spark works and take advantage of this for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mouse = salesData.where($\"Product\" === \"Mouse\")\n",
    "\n",
    "println(\"-------------------------------EXPLAIN Filter by Partition------------------------------------\")\n",
    "mouse.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n",
    "\n",
    "\n",
    "val google=salesData.where( $\"Customer\" === \"Google\")\n",
    "\n",
    "println(\"-------------------------------EXPLAIN Filter without Partition------------------------------------\")\n",
    "google.explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")\n",
    "\n",
    "println();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------EXPLAIN Filter by Partition------------------------------------\n",
    "- Partion count is 1 as it is a partitioned column  \n",
    "- so Spark needs to read only one Partition and load it, this is most efficient  \n",
    "== Physical Plan ==  \n",
    "*(1) FileScan parquet [ID#297,Customer#298,Date#299,Quantity#300,Rate#301,Tags#302,Product#303] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 1, PartitionFilters: [isnotnull(Product#303), (Product#303 = Mouse)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
    "-------------------------------END EXPLAIN--------------------------------\n",
    "\n",
    "-------------------------------EXPLAIN Filter without Partition------------------------------------\n",
    "- Here PartitionCount is 4    \n",
    "- Reading Non partitioned column involves reading all data blocks but load only those are remaining after filter  \n",
    "== Physical Plan ==  \n",
    "*(1) Project [ID#297, Customer#298, Date#299, Quantity#300, Rate#301, Tags#302, Product#303]\n",
    "+- *(1) Filter (isnotnull(Customer#298) && (Customer#298 = Google))\n",
    "   +- *(1) FileScan parquet [ID#297,Customer#298,Date#299,Quantity#300,Rate#301,Tags#302,Product#303] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,Google)], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
    "-------------------------------END EXPLAIN--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.Partitioning and coalescing\n",
    "- While performing actions, Spark creates results with the default partition count. In the case of Local mode, its usually equal to the number of cores. In the case of Clusters, the default is 200. This can be too much, if the number of cores in the cluster is significantly less than the number of partitions. So repartitioning helps to set the optimal number of partitions.\n",
    "\n",
    "- Repartition does a full reshuffle and can be used for increasing/decreasing partitions.\n",
    "\n",
    "- Coalasce simply consolidates existing partitions and avoids a full reshuffle. It can be used to decrease the number of partitions.\n",
    "\n",
    "- Repartition and Coalasce themselves take significant time and resources. Do them only if multiple steps downstream will benefit from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Default parallelism : \" + sc.defaultParallelism + \"\\n\")\n",
    "//Optimal number of partitions = # of cores available.\n",
    "\n",
    "println(\"Partitions in SalesData from Parquet : \" + salesData.rdd.getNumPartitions + \"\\n\")\n",
    "\n",
    "//Read file without parallelizing\n",
    "val rawSalesData = spark\n",
    "                .read\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"/user/raj_ops/raw_data/sales_orders.csv\");\n",
    "                \n",
    "println(\"Partitions in raw CSV Read :\" + rawSalesData.rdd.getNumPartitions + \"\\n\")\n",
    "\n",
    "//Repartition to 8 partitions\n",
    "val partitionedSalesData = rawSalesData.repartition(8)\n",
    "\n",
    "println(\"Partitions after repartitioning :\" + partitionedSalesData.rdd.getNumPartitions + \"\\n\")\n",
    "\n",
    "//Coalesce to 3 partitions\n",
    "val coalasedSalesData = partitionedSalesData.coalesce(3)\n",
    "\n",
    "println(\"Partitions after coalese :\" + coalasedSalesData.rdd.getNumPartitions + \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Default parallelism : 2\n",
    "\n",
    "Partitions in SalesData from Parquet : 2\n",
    "\n",
    "Partitions in raw CSV Read :1\n",
    "\n",
    "Partitions after repartitioning :8\n",
    "\n",
    "Partitions after coalese :3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.Managing Shuffling\n",
    "- Actions trigger shuffling. Shuffling takes time, memory and bandwidth. While building pipelines focus on\n",
    "\n",
    "    - Minimize number of shuffles\n",
    "    - Do actions late in the pipeline after data has been filtered.\n",
    "    - Use aggregations by partition key as much as possible, as records with the same partition key stays in the same executor node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordsRDD=spark.sparkContext\n",
    "                .parallelize(Seq(\"Google\", \"Apple\", \"Apple\", \"Google\", \n",
    "                    \"Google\", \"Apple\", \"Apple\", \"Apple\", \"Apple\", \"Apple\"))\n",
    "                    \n",
    "\n",
    "#Doing groupBy first. Shuffling has more data. Check DAG\n",
    "\n",
    "print(\"For using group by key : \")\n",
    "val groupRDD = spark.time(\n",
    "                wordsRDD.map( word => (word, 1) )\n",
    "                    .groupByKey()\n",
    "                    .map( words => (words._1, words._2.sum ))\n",
    "                    .collect())\n",
    "\n",
    "#Doing reduce. Shuffling has less data. Check DAG\n",
    "print(\"For using reduce : \")\n",
    "var reduceRDD = spark.time(\n",
    "                wordsRDD.map( word => (word, 1) )\n",
    "                    .reduceByKey(_+_).collect())\n",
    "\n",
    "                \n",
    "#See content generated by groupByKey and reduceByKey\n",
    "println(\"\\nData shuffled after Group by Key : \")\n",
    "wordsRDD.map( word => (word, 1) )\n",
    "                    .groupByKey()\n",
    "                    .collect()\n",
    "                    .foreach(println)\n",
    "\n",
    "println(\"\\nData shuffled after Reduce by Key: \")                    \n",
    "wordsRDD.map( word => (word, 1) )\n",
    "                    .reduceByKey(_+_)\n",
    "                    .collect()\n",
    "                    .foreach(println)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For using group by key : Time taken: 3242 ms\n",
    "For using reduce : Time taken: 1803 ms\n",
    "\n",
    "Data shuffled after Group by Key : \n",
    "(Apple,CompactBuffer(1, 1, 1, 1, 1, 1, 1))\n",
    "(Google,CompactBuffer(1, 1, 1))\n",
    "\n",
    "Data shuffled after Reduce by Key: \n",
    "(Apple,7)\n",
    "(Google,3)\n",
    "wordsRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[86] at parallelize at <console>:24\n",
    "groupRDD: Array[(String, Int)] = Array((Apple,7), (Google,3))\n",
    "reduceRDD: Array[(String, Int)] = Array((Apple,7), (Google,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. Optimizing Joins\n",
    "- By default, joining two data frames require a lot of shuffling. If one data frame is considerably small, a better option is to broadcast that data frame to all the executors and then use those copies to join locally. Spark Optimizer chooses Broadcast joins when possible. Data frames within spark.sql.autoBroadcastJoinThreshold are automatically broadcasted\n",
    "- recommended to use denormalize data and avoid joins if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val products = spark\n",
    "                .read\n",
    "                .option(\"inferSchema\", \"true\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"/user/raj_ops/raw_data/product_vendor.csv\");\n",
    "                \n",
    "products.show()\n",
    "\n",
    "import org.apache.spark.sql.functions.broadcast\n",
    "println(\"-------------------------------EXPLAIN------------------------------------\")\n",
    "salesData.join(broadcast(products),\"Product\").explain\n",
    "println(\"-------------------------------END EXPLAIN--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------EXPLAIN------------------------------------\n",
    "== Physical Plan ==\n",
    "*(2) Project [Product#303, ID#297, Customer#298, Date#299, Quantity#300, Rate#301, Tags#302, Vendor#380]\n",
    "+- *(2) BroadcastHashJoin [Product#303], [Product#379], Inner, BuildRight\n",
    "   :- *(2) FileScan parquet [ID#297,Customer#298,Date#299,Quantity#300,Rate#301,Tags#302,Product#303] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/partitioned_parquet/Produc..., PartitionCount: 4, PartitionFilters: [isnotnull(Product#303)], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
    "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n",
    "      +- *(1) Project [Product#379, Vendor#380]\n",
    "         +- *(1) Filter isnotnull(Product#379)\n",
    "            +- *(1) FileScan csv [Product#379,Vendor#380] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/product_vendor.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Product)], ReadSchema: struct<Product:string,Vendor:string>\n",
    "-------------------------------END EXPLAIN--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06.Caching in Spark/Storing Intermediate Results\n",
    "- By default, every time an action is performed, Spark executes all the previous steps right from the data read. This can end up being very expensive, especially while using Spark in a development or interactive mode. A better option is to cache intermediate results. \n",
    "- Two types of Caching\n",
    "    - Caching - Spark can cache in memory. \n",
    "    - Persistance - It can also persist in both memory and disk. While running under YARN, persistance happens in HDFS by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In memory only\n",
    "wordsRDD.cache()\n",
    "#Trigger an action for caching\n",
    "wordsRDD.collect()\n",
    "\n",
    "println(\"\\nPlan before caching intermediate results :\")\n",
    "val dataBefore = coalasedSalesData.where($\"Product\" === \"Mouse\")\n",
    "dataBefore.explain\n",
    "\n",
    "#Store on disk\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "coalasedSalesData.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "#Trigger an action for persisting.\n",
    "coalasedSalesData.count()\n",
    "\n",
    "println(\"\\nPlan after caching :\")\n",
    "val dataAfter = coalasedSalesData.where($\"Product\" === \"Mouse\")\n",
    "dataAfter.explain"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plan before caching intermediate results :\n",
    "== Physical Plan ==\n",
    "Coalesce 3\n",
    "+- Exchange RoundRobinPartitioning(8)\n",
    "   +- *(1) Project [ID#329, Customer#330, Product#331, Date#332, Quantity#333, Rate#334, Tags#335]\n",
    "      +- *(1) Filter (isnotnull(Product#331) && (Product#331 = Mouse))\n",
    "         +- *(1) FileScan csv [ID#329,Customer#330,Product#331,Date#332,Quantity#333,Rate#334,Tags#335] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/sales_orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Product), EqualTo(Product,Mouse)], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
    "\n",
    "Plan after caching :\n",
    "== Physical Plan ==\n",
    "*(1) Filter (isnotnull(Product#331) && (Product#331 = Mouse))\n",
    "+- InMemoryTableScan [ID#329, Customer#330, Product#331, Date#332, Quantity#333, Rate#334, Tags#335], [isnotnull(Product#331), (Product#331 = Mouse)]\n",
    "      +- InMemoryRelation [ID#329, Customer#330, Product#331, Date#332, Quantity#333, Rate#334, Tags#335], true, 10000, StorageLevel(disk, 1 replicas)\n",
    "            +- Coalesce 3\n",
    "               +- Exchange RoundRobinPartitioning(8)\n",
    "                  +- *(1) FileScan csv [ID#329,Customer#330,Product#331,Date#332,Quantity#333,Rate#334,Tags#335] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://sandbox-hdp.hortonworks.com:8020/user/raj_ops/raw_data/sales_orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Best Practices\n",
    "- Push down filters and projections to data sources as much as possible\n",
    "- Choose partition keys , based on columns most used for filters and aggregations\n",
    "- Repartition or Coalesce only if multiple transforms\n",
    "- Avoid joins and use denormalize data\n",
    "- Time operations using spark.time() on production equivalent data\n",
    "- Use Caching when appropriate \n",
    "- Use Explain to understand the physical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
