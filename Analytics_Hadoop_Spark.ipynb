{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "- Open Source\n",
    "- Distributed store and computing\n",
    "- Can scale to petabytes of data\n",
    "- Consists of\n",
    "    - Hadoop Distributed File System (HDFS)\n",
    "    - MapReduce\n",
    "    \n",
    "Source: LinkedIn: big-data-analytics-with-hadoop-and-apache-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical on \n",
    "- Ambari sandbox\n",
    "- Zeppelin notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Distributed File System (HDFS)\n",
    "- good and cheap option to store large amount of data\n",
    "- provides scaling, security and cost benefit\n",
    "- suitable for enterprises with in-house data centers\n",
    "- Cloud Alternatives - Amazon S3, Oracle OSS, Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "- Scales Horizontally\n",
    "- Very Slow as it uses disk storage for internediate caching instead of memory\n",
    "- Faster Alternatives - Apache Spark, Apache Flink\n",
    "- These alternatives also has growing list of supporting libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "- Open Source\n",
    "- Large scale distributed data processing engine\n",
    "- Uses memory to speed up computations\n",
    "- Batch, streaming, ML and graph capabilities\n",
    "- Support Scala, Java, Python and R \n",
    "- Most popular big data platform today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop and Spark\n",
    "- Spark is well integrated with Hadoop\n",
    "- Spark can access and process data HDFS using \n",
    "    - parallel nodes\n",
    "    - read optimization to use less memory and I/O\n",
    "    - use HDFS for intermediate Data Caching\n",
    "    - Yarn provides single cluster management for both HDFS and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Data Modelling for Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Hadoop Storage Formats\n",
    "- Raw Texts (blobs)\n",
    "- Structured text files (csv, xml, json)\n",
    "- Sequence Files\n",
    "- Avro\n",
    "- ORC\n",
    "- Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Files:  \n",
    "- Simple to read/write\n",
    "- Low performance - no parallel operations\n",
    "- More Storage\n",
    "- No schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avro:  \n",
    "- Language neutral data serialization\n",
    "- Row format\n",
    "- Self-describing schema support\n",
    "- Compressible\n",
    "- Splittable\n",
    "- Ideal for multi-language support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet:  \n",
    "- Columnar Format\n",
    "- Read-only selected columns (saves I/O)\n",
    "- Schema Support\n",
    "- Compressible (column level) and Splittable\n",
    "- Supports nested Data Structures\n",
    "- Ideal for analytics applications  \n",
    "\n",
    "Parquet for Analytics:  \n",
    "- provides overall better performance and flexibility for analytics applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hadoop Compression Options\n",
    "- Snappy\n",
    "    - Compression codec developed by google\n",
    "    - Moderate Compression\n",
    "    - Excellent read/write performance\n",
    "    - Compresses entire file\n",
    "    - Not splittable so dont support parallel operations\n",
    "- LZO\n",
    "    - Moderate compression\n",
    "    - Excellent processing performance\n",
    "    - Splittable - Support parallel processing\n",
    "    - requires separate license\n",
    "- GZIP\n",
    "    - Very good compression\n",
    "    - Moderate processing performance\n",
    "    - Not Splittable\n",
    "    - Ideal for container type applications\n",
    "- bzip2\n",
    "    - Excellent compression\n",
    "    - Slower processing performance \n",
    "    - Splittable\n",
    "    - Ideal for archival type applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Partitioning\n",
    "- HDFS does not have the concept of indexes\n",
    "- Even for reading one row, the entire file should be read\n",
    "- Partitioning provides a way to read only a subset of data\n",
    "- Multiple attributes can be used for hierarchical partitioning\n",
    "- Split data into directories based on individual values of attributes\n",
    "\n",
    "<img src=\"Image/partitioning.JPG\" width=\"600\" />\n",
    "\n",
    "- choose attributes with a limited set of values and those that are most used in SELECT filters\n",
    "- otherwise many sub directories will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bucketing\n",
    "- Partitioning is optimal when an attribute is having small set of unique values\n",
    "- What if we need to partition based on a key having large no of values\n",
    "- Similar to Partioning but instead of value it uses a Hash function to convert value to a Hash key\n",
    "- Controls number of unique directories created\n",
    "- Even disribution\n",
    "- Choose attributes with large number of unique values and those that are most used in SELECT filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. HDFS Schema Designing and Storage Best Practices\n",
    "- understand the data whether its read intensive or write intensive or both\n",
    "- determine what needs optimization and what can be compromised (reduce storage req or compromise storage for better read/write performance)\n",
    "- choose options carefully as they cant be changed easily\n",
    "- run tests on actual data to understand performance and storage caracteristics\n",
    "- choose partitioning and bucketing keys wisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction with Spark Best Practices\n",
    "- reduce data read into memory by:\n",
    "    - using filters based on partition key\n",
    "    - reading only required columns\n",
    "- use data sources and file formats that support parallelism\n",
    "- keep number of partitions >= (No. of executors * No. of cores per executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
