{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkedin: apache-spark-essential-training-big-data-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineering**  \n",
    "- focus on data\n",
    "- Capture, movement, storage , security and processing\n",
    "- convert Raw data into knowledge data \n",
    "\n",
    "\n",
    "**Data Engineers**\n",
    "- Build data pipelines, applications and APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineering Pipeline**\n",
    "- Stages\n",
    "1. Acquisition\n",
    "2. Transport\n",
    "3. Storage\n",
    "4. Processing\n",
    "5. Servicing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aquisition\n",
    "- Format of data\n",
    "- Interfaces available to get access to data\n",
    "- Security (authorisation, authentication, encryption)\n",
    "- Reliability\n",
    "- Latency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transport\n",
    "- Reliability and Integrity\n",
    "- Security\n",
    "- Latency\n",
    "- Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Storage\n",
    "- Flexibility and Ease of processing (keep data in native form or need to summarize it)\n",
    "- Schema or Schema Less design\n",
    "- High Availability/ Redundancy\n",
    "- Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Processing\n",
    "- Cleaning to remove inconsistency and bad data\n",
    "- Filtering - choosing relevant data\n",
    "- Enriching (data joints and denormalization)\n",
    "- Aggregating\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Servicing\n",
    "- Latency\n",
    "- Redundancy and High Availability\n",
    "- Skill Levels of consumers\n",
    "- Flexibility of schema\n",
    "- APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BigData Classified on below attributes\n",
    "- Volume (Resources req, scalable, maintaining latency)\n",
    "- Velocity (real-time event data handling, need for speed, handling lags)\n",
    "- Variety (Text, audio, video and images, more resources needed, Serving at low latency)\n",
    "- Variability (spikes in load, decoupling needed with buffering zones, maintaining latency)\n",
    "\n",
    "#### Pipeline\n",
    "- Functionality\n",
    "- Speed\n",
    "- Reliability\n",
    "- Security\n",
    "- Availability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark - Data processing engine  \n",
    "Apache Kafka - Data Aquisition and transport Layer  \n",
    "HDFS/MySQL - Storage  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Kafka Connect**\n",
    "- scalable distributed pipeline for moving data\n",
    "- Data Source -> Kafka -> Data Sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "- best tool for data engineering  \n",
    "\n",
    "Advantages:  \n",
    "1. Built as a compute engine\n",
    "2. Faster Data processing\n",
    "3. Massive Horizontal scalability\n",
    "4. Streaming Support\n",
    "5. Machine Learning Libraries (pyspark)\n",
    "6. Third Party integrations \n",
    "\n",
    "Features:  \n",
    "1. Spark Transformations - record level processing in distributed fashion, used in (1. Data Cleansing, 2. Data Validation, 3. Filtering, 4. Joining and Enriching Data, 5. Aggregation)\n",
    "2. Spark Actions - extract meaningful information from massive dataset (1. Metrics, 2. Aggregate data to provide summaries, 3. Moving data to external systems like File system and databases)\n",
    "3. Spark Broadcast variables and accumulators - minimize data across network and generalize system wide metrics(1. Lookup tables, 2. Shared variables, 3. Summary Metrics, 4. Data Consolidation)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Spark Works? - Stages\n",
    "1. Acquire Data\n",
    "2. Create RDD\n",
    "3. Transformation\n",
    "4. Shuffle\n",
    "5. Action to store\n",
    "\n",
    "<img src=\"Image/spark-transformation.JPG\" width=\"600\" />\n",
    "\n",
    "- No Shuffle - functions those run on individual records in the dataset independently do not create shuffle (e.g: Map, Filter, flatMap, mapPartitions)\n",
    "- Shuffle Transformations - functions which require data to be consolidated in some fashion and cross referenced between RDDs for this consolidations (e.g: Distinct, groupByKey, reduceByKey, Join)\n",
    "- Actions to move data from cluster back to single driver node - need to use minimal data to avoid huge network traffic (e.g: Reduce, Collect, Count, saveAsTextFile)\n",
    "\n",
    "Lazy Evaluation  \n",
    "- Spark executes transformations only when an action is executed on the resulting RDDs\n",
    "- Spark optimizes execution of all statements in the batch when it executes the action.\n",
    "- The more statements to execute, the better the chance to optimize  \n",
    "\n",
    "\n",
    "To take adv of Lazy Evaluation  \n",
    "- execute code with action\n",
    "- do as many transformations as possible before hitting an action\n",
    "- avoid debugging statements like \"print count\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Spark Dependencies?  \n",
    "- When transformation is executed and a RDD is created from another RDD\n",
    "- Does the transformation result in shuffle\n",
    "    - Wide Dependency - Yes Shuffle (E.g: RDD13 need data from both worker node)\n",
    "    - Narrow Dependency - No Shuffle (E.g: RDD11 -> RDD12)\n",
    "    - Wide Dependency cause data to flow between worker node, which is expensive and time consuming\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize for Dependencies?\n",
    "- do as many Narrow Dependencies as possible before hitting a Wide Dependency\n",
    "- try to group Wide Dep. together, possibly in a single function and do it once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators\n",
    "- standard accumulator allow only single value like integer, string to pass around between driver programs and the nodes\n",
    "- to pass multiple values like list, write your own accumulator implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "1. What is driver and executor in Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
