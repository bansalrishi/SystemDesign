{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges with RDBMS\n",
    "- As data grows it becomes slow\n",
    "- Expensive\n",
    "\n",
    "ETL (Extract, Transform, Load) and Data Warehouse\n",
    "- These tools transfer data in various formats to Data warehouse for Analytics and Reporting purpose\n",
    "- Teradata is leading vendor for Data Warehouse Solution\n",
    "- Can't use analytics in Data Warehouse real time (e.g: suggest a product to a customer who is buying a certain product)\n",
    "- ETL - BODS(SAP Labs)\n",
    "\n",
    "Hadoop\n",
    "- Platform which takes the space of Data Warehouse\n",
    "- Instead of ETL we use different product to read different kind of data type (e.g: SQL data, xml, json, logs, etc)\n",
    "- Apache Sqoop we use to read SQL \n",
    "- Open Source - Cloudera, Hortonworks , MapR & Amazon Elastic MapReduce are major Provider\n",
    "- User can connect through a Gateway or Hadoop Client, no direct access to Hadoop cluster\n",
    "- Its having NodeName (Master) and DataName(Slave). Slaves are on cheap hardware\n",
    "- Default lock size is 128MB\n",
    "    - it can be increased for less metadata on NodeName server\n",
    "- ELT\n",
    "    - Extraction - For Structured Data (oracle, mysql) - Sqoop\n",
    "    - Apache Flume - For UnStructured Data (xml, json, flat file) - point-to-point data delivery tool, dont save any data\n",
    "    - Apache Kafka - can take data from Flume as Kafka producer should be present on source to get data, for Flume no such requirement\n",
    "- UnStructured Data -> Flume -> Kafka -> (HDFS for storage, Spark Streaming for real time processing)\n",
    "    - take adv of features of Flume such as bucketing and event modification / routing, Kite SDK Morphline Integration, and NRT indexing with Cloudera Search\n",
    "- Spark Streaming/Flink/Storm\n",
    "    \n",
    "- Direct copy from oracle DB not allowed\n",
    "    - Oracle Golder Gate (Change Data Capture, CDC) - Capture data from replica logs, it provides changes in the data\n",
    "- MapReduce/Pig/HIVE/Spark for batch processing data in HDFS\n",
    "- Hive - sql query converts to program for MapReduce\n",
    "    - How MapReduce takes care in event of server failure?\n",
    "- IMPALA/HAWQ,LLAP/PRESTO/DRILL/PHOENIX - for small query, in memory - can fail if its running on a server which goes off\n",
    "    - in case of failure does it retry?\n",
    "- Hive - query wont fail if any server goes off\n",
    "- HBase is a Database on HDFS - random read/write DB - own language\n",
    "    - Can we replace Oracle DB by this?\n",
    "- Phoenix - sql queries can be run, put on top of Hbase\n",
    "\n",
    "<img src=\"Image/hadoop.JPG\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pig(Yahoo) and Hive(Facebook)\n",
    "- Hive convert Sql Queries to MapReduce Code\n",
    "- Hive is like Database of HDFS, Spark can also talk to Hive\n",
    "- Tez on Hortonworks same like Hive but Hive is very slow as it uses MapReduce\n",
    "- IMPALA is on Cloudera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN\n",
    "The two main components of YARN are–\n",
    "\n",
    "- ResourceManager– This component receives processing requests and accordingly allocates to respective NodeManagers depending on processing needs.\n",
    "- NodeManager– It executes tasks on each single Data Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive\n",
    "- provides SQL like interface to Hadoop\n",
    "- dont have storage\n",
    "- project a structure to Data which is in Hadoop\n",
    "- can read different kind of data e.g: xml, json, etc\n",
    "- convert sql query into MapReduce code\n",
    "- Hadoop dont understand what is Hive\n",
    "- its a Dataware House\n",
    "- HQL - Hive Query Language\n",
    "- Hive is not a database\n",
    "- take minutes even for small queries\n",
    "- doesnt provide real time queries and row level updates\n",
    "- not suitable for Online Transaction Processing(OTP) systems\n",
    "- LLAB feature of Hive gives real time performance but not as reliable as RDBMS\n",
    "<img src=\"Image/hive.JPG\" width=\"600\" />\n",
    "\n",
    "- cbo - cost based optimizer -for complicated queries, it will generate plans and select the least expensive plan to execute the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive - Commands  \n",
    "- describe formatted TABLENAME;\n",
    "    - location of data\n",
    "    - MANAGED_TABLE\n",
    "- two kind of tables\n",
    "    - managed - we dont have control where data will be stored\n",
    "        - when you drop a table, data will go as well, it is managed by Hive only\n",
    "    - external - we control where to store data\n",
    "        - data wont be lost as Hive has no control over Hadoop\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQL is more optimized, we understand our data due to schema. So, if I have filter and join, I will push filter first to optimize my query. By default filter comes first.\n",
    "- This is not possible in Spark, but Spark Sql may be possible.\n",
    "- MapReduce also loads full file, so not optimized\n",
    "- HTTP Response Code\n",
    "    - 1xx (Informational Response)\n",
    "    - 2xx (Sucess)\n",
    "    - 3xx (Redirection)\n",
    "    - 4xx (Client Error)\n",
    "    - 5xx (Server Error)\n",
    "-Hive built-in SerDe (serializer/deserializer)\n",
    "    - create table if not exist and than store as text file using SerDe RegEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPALA\n",
    "- demon - impala d - run on all nodes\n",
    "- has a shell, can fire a query, dont depend on Hive\n",
    "- can see table in Hive\n",
    "- it dont depend on MapReduce, Impala D run the query\n",
    "- run query in-memory\n",
    "- Impala query is not fault tolerant, but query is very fast. If one server fails, query will go and the job need ot be started again\n",
    "- Regex not supported in IMPALA , so table created by Hive using regex - Impala cant run query on this table\n",
    "- refresh need to be done manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning in Hive\n",
    "- static and dynamic partitioning\n",
    "    - the folder will be having lot of files for over a period of time\n",
    "    - create a partitioning table with column is month\n",
    "    - its useful when cardiniality(value) is very less\n",
    "    - its effective in Hadoop but in Oracle its not very effective\n",
    "        - suppose partitioning is on basis of country and data is in TB for US, in Hadoop query will run on many servers but in case of oracle its run on only one server\n",
    "    - Static - we can partition suppose on country and region but these columns are not there\n",
    "    - Dynamic - columns exist, dynamic partitioning is disable by default\n",
    "    - we cant directly load data in Dynamic partitioning, first we need to load into table and than into partitioning table\n",
    "    - While enabling dynamic partioning one static partition need to be created, check why?\n",
    "    - Dynamic - Can put restriction on number of partition for safety\n",
    "- Physical partitioning like if there is 100 rows divide into 5, this is impossible in RDBMS but possible in nosql as data is denormalized\n",
    "- Drawbacks of Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing in Hive\n",
    "- divide data based on Hash value\n",
    "- e.g: bucket = 10, 10 files will be created\n",
    "- we can create bucketing without partitioning\n",
    "- decide bucket size based on no. of block, dont make very small\n",
    "- bucket joins - advantage- its faster\n",
    "- mapper and reducer?\n",
    "- reducer - when there is shuffle operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General facts:  \n",
    "- Query results can be saved in Hadoop\n",
    "- We can specify how to store data while creating table\n",
    "- Data will be in text file format e.g: **ORC** (it has indexing with compression), Parquet, RC\n",
    "- ORC - Vectorization, read thousands of Row in one time\n",
    "- ORC - Drawback - it needs decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "- 2009 ->AMPLab Berkley -> MESOS (cluster manager similar to YARN)\n",
    "- Spark was created to test Mesos, in-memory\n",
    "- Spark programs was running faster than MapReduce\n",
    "- 1.6 (Stable)\n",
    "- 2.3 Latest\n",
    "- It doesnt need Hadoop, can run independently\n",
    "- Spark The Definite Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image/hadoop_his.JPG\" width=\"600\" />\n",
    "\n",
    "- Machine Learning is iterative, for which Hadoop is not that good, is was very slow as it uses MapReduce\n",
    "- Adv of Spark it can do all which above tools can do, so learning Spark is enough for all functions\n",
    "- Creadit Card - identify transaction is fraud or not. In Spark streaming, we can do live processing but it cant do for each transaction. It can do batch processing say per sec. Apache Storm can do per transaction processing\n",
    "\n",
    "## Spark Architecture\n",
    "<img src=\"Image/spark_arc1.JPG\" width=\"600\" />\n",
    "\n",
    "\n",
    "- Scheduling, Monitoring, Distributing\n",
    "- Can control RAM, Harddisk\n",
    "- Support 4 Languages - Scala, Java, Python or R\n",
    "- Core Spark - like MapReduce\n",
    "- We get libraries - SQL, Streaming, MLlib, GraphX\n",
    "- Can read Hive table which is very fast, HIve is used as storage, and query is ran by Spark SQL\n",
    "- Graph processing supported\n",
    "- Neo4j - Database for Graphs\n",
    "- Running Mode - locally, Mesos Clusters, Standalone Mode, Yarn - except locally all supports zookeeper, which ensures high availability\n",
    "- No storage, its an execution engine\n",
    "- Can also run on Kubernetes\n",
    "- Can read from any File System, any DB\n",
    "- E.g: Get data from Twitter, so if any server goes down chances of data loss will be there for few seconds. So, we can use Flume or Kafka to avoid data loss from Twitter as Kafka will recieve messages from Twitter.\n",
    "<img src=\"Image/hadoop_latest.JPG\" width=\"400\" />\n",
    "\n",
    "Why Spark is better than MapReduce?  \n",
    "<img src=\"Image/mapreduce_spark.JPG\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Driver -> Master  \n",
    "Executer -> Slave  \n",
    "Local Execution -> JVM or container will be created in YARN which is having Driver and Executer both.  \n",
    "Cluster Mode -> Suppose Spark will ask for 4 executers, YARN will allocate 4 executers, master will push the Spark code to each executer container which is running on different server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs (Resilient Distributed Datasets) Fundamentals\n",
    "- in Spark data is represented as Spark\n",
    "- RDD represent data in RAM\n",
    "- Suppose data is residing in 4 blocks on different server, YARN allocates 4 executers, but YARN dont know which server data actually resides unlike MapReducer.\n",
    "<img src=\"Image/rdd.JPG\" width=\"400\" />  \n",
    "- Partitioning will be equal to no. of data blocks if data is inside Hadoop\n",
    "- if data is in not Hadoop but cassandra nosql db, than it can be divided into any number of partition as per our input\n",
    "- Executer can process more than one partition\n",
    "- for single JVM container minimum one processor core is needed\n",
    "- if you ask for 10GB RAM for an executer, you will get around 5.8GB. 10% container keeps for itself(system calls, communication with Server and YARN, etc) andin reamining 40% if for jvm java garbage collection,etc. So, in effect only around 5.8GB is allocated\n",
    "- Once data is in RDD, we can start writing Spark function\n",
    "- transformation - map, filter, flatMap, mapPartitions, etc  \n",
    "<img src=\"Image/rdd1.JPG\" width=\"400\" />\n",
    "- RDD is immutable\n",
    "- use of coalesce will reduce number of executers so in that way we can avoid issue of executer sitting ideal  \n",
    "<img src=\"Image/rdd2.JPG\" width=\"400\" />\n",
    "- without Actions (.collect is an action) nothing happen  \n",
    "<img src=\"Image/rdd3.JPG\" width=\"400\" />\n",
    "- DAG - Directed Acyclic Graph\n",
    "\n",
    "**RDD Cache**  \n",
    "<img src=\"Image/rdd4.JPG\" width=\"400\" />  \n",
    "- if multiple actions are there, the execution will start everytime from start for each action. To avoid this , we can use RDD cache, it will cache intermediate results and Cache will be cleared once Spark program is completed  \n",
    "\n",
    "\n",
    "**Issue with RDD**  \n",
    "- until Spark runs the code it dont know what to optimize\n",
    "- so we have Spark SQL to optimize the code\n",
    "\n",
    "- E.g: #pyspark2 --master yarn --num-executers 3\n",
    "- To check executors, config, etc -> #sc._conf.getALL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://blog.cloudera.com/flafka-apache-flume-meets-apache-kafka-for-event-processing/\n",
    "**Example: Event Processing During Ingest**  \n",
    "- Build a pipeline for performing arbitrary event processing. \n",
    "- Flume provides a key component called the interceptor\n",
    "- Interceptors have the following characteristics:\n",
    "    - Inspect events as they pass between source and channel\n",
    "    - Modify or drop events as required\n",
    "    - Be chained together to form a processing pipeline\n",
    "    - Execute any custom code within the event processing\n",
    "- Can use Flume interceptors to do a variety of processing against incoming events as they pass through the system\n",
    "- E.g: calculating a simple “Travel Score” -  to identify whether a banking customer is traveling while using their debit card\n",
    "- Other use case of Interceptor:\n",
    "    - Inspecting the content of the message for proper routing to a particular location such as by geo region\n",
    "    - Calculating a streaming TopN list\n",
    "    - Callout to a machine learning serving layer\n",
    "    - Event enrichment / augmentation\n",
    "    - In-flight data masking\n",
    "    \n",
    "- Note: For complex stream processing - Spark Streaming \n",
    "    - Flume Interceptors provide a great way to process events with very low latency and minimal complexity. For per-event response latencies under 50 ms, building a custom application is the right choice\n",
    "    \n",
    "- To do any meaningful processing of the event as it arrives, you need to enrich the incoming transaction with information from your other systems\n",
    "    - call Apache HBase to get additional values related to the transaction and modify the record to reflect the results of the processing performed by Interceptor\n",
    "    <img src=\"Image/flume.JPG\" width=\"600\" /> \n",
    "    - You can write your event directly to HDFS as before or back to Kafka, where the event could be picked up by other systems or for more comprehensive stream processing. In this case, you’ll return it directly back to Kafka so that the authorization result can be immediately returned to the client\n",
    "    \n",
    "- The Flume memory channel does not protect against data loss in the event of agent failure, and the when using the file channel, any data in a channel not yet written to a sink will be unavailable until the agent is recovered. The Kafka channel addresses both of these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Security\n",
    "Hadoop security needs to be individually implemented across various architecture layers\n",
    "1. Physical access — Individual Hadoop server hosts and HTTP web UIs presented by the various Hadoop technologies need to be protected to ensure that only the required users have access to the respective underlying components, the right hosts and the right ports. This is achieved by means of network firewalls and server netgroups.\n",
    "\n",
    "2. Authentication — Most Hadoop components allow for two-different kinds of authentication mechanisms: Kerberos and LDAP. To this end, the solution needs to be integrated with the right KDC and LDAP database to verify end-user credentials and also Hadoop service credentials at each and every interchange and hand-off that happens across the distributed solution spanning several hosts.\n",
    "    - Kerberos and LDAP\n",
    "\n",
    "3. Authorization — After ensuring that a user has the right of access to a service and is indeed the rightful user, the next step of security deals with verifying that the user has the right access grants for whatever they are attempting to do in the Hadoop ecosystem. This authorization is currently not uniform across the Hadoop stack and a lot of services require their own authorization configurations using Access Control Lists and by means of some popular Hadoop authorization tools like Sentry and Ranger.\n",
    "    - Access Control List\n",
    "    - Tools - Sentry and Ranger\n",
    "\n",
    "4. Encryption — Hadoop security also requires that data be protected both at rest and in transit. Data in transit is secured by means of manually applying public/private key encryption between each of the hosts that makes up the Hadoop ecosystem. Data at rest solutions are individually presented by some of the Hadoop services directly like KMS/KTS for HDFS, but are also possible at the actual physical storage level using tools like NavEncrypt.\n",
    "    - Data be protected both at rest and in transit\n",
    "    - In transit - manually apply public/private key encryption b/w each hosts\n",
    "    - Data at rest - NavEncrypt at physical layer\n",
    "\n",
    "5. Governance — While all the above layers ensure that only the right user can reach your services, validate himself against them, verify his access to use them and leverage them securely using encryption, all of this does not guarantee protection against any misses in implementation or execution. Data governance allows for active monitoring of the individual transactions and data entities and operation in the Hadoop ecosystem and is a powerful tool to make sure that the platform is being leveraged as intended. Cloudera Navigator is example of one such tool that can be leveraged to this end. Manual solutions can also be constructed.\n",
    "    - Cloudera Navigator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "- What do you understand by Rack Awareness in Hadoop?\n",
    "    - It is an algorithm applied to the NameNode to decide how blocks and its replicas are placed. Depending on rack definitions network traffic is minimized between DataNodes within the same rack. For example, if we consider replication factor as 3, two copies will be placed on one rack whereas the third copy in a separate rack.\n",
    "\n",
    "- What is the difference between “HDFS Block” and “Input Split”?\n",
    "    - The HDFS divides the input data physically into blocks for processing which is known as HDFS Block. Input Split is a logical division of data by mapper for mapping operation.\n",
    "    \n",
    "-  What are the common input formats in Hadoop?\n",
    "    - Below are the common input formats in Hadoop –\n",
    "    - Text Input Format – The default input format defined in Hadoop is the Text Input Format.\n",
    "    - Sequence File Input Format – To read files in a sequence, Sequence File Input Format is used.\n",
    "    - Key Value Input Format – The input format used for plain text files (files broken into lines) is the Key Value Input Format.\n",
    "    \n",
    "- What are the different configuration files in Hadoop?\n",
    "   - The different configuration files in Hadoop are –\n",
    "\n",
    "    - core-site.xml – This configuration file contains Hadoop core configuration settings, for example, I/O settings, very common for MapReduce and HDFS. It uses hostname a port.\n",
    "\n",
    "    - mapred-site.xml – This configuration file specifies a framework name for MapReduce by setting mapreduce.framework.name\n",
    "\n",
    "    - hdfs-site.xml – This configuration file contains HDFS daemons configuration settings. It also specifies default block permission and replication checking on HDFS.\n",
    "\n",
    "    - yarn-site.xml – This configuration file specifies configuration settings for ResourceManager and NodeManager.\n",
    "    \n",
    "- What is the use of jps command in Hadoop?\n",
    "    - The jps command is used to check if the Hadoop daemons are running properly or not. This command shows all the daemons running on a machine i.e. Datanode, Namenode, NodeManager, ResourceManager etc.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
