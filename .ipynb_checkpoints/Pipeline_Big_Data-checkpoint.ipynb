{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkedin: apache-spark-essential-training-big-data-engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineering**  \n",
    "- focus on data\n",
    "- Capture, movement, storage , security and processing\n",
    "- convert Raw data into knowledge data \n",
    "\n",
    "\n",
    "**Data Engineers**\n",
    "- Build data pipelines, applications and APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Engineering Pipeline**\n",
    "- Stages\n",
    "1. Acquisition\n",
    "2. Transport\n",
    "3. Storage\n",
    "4. Processing\n",
    "5. Servicing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aquisition\n",
    "- Format of data\n",
    "- Interfaces available to get access to data\n",
    "- Security (authorisation, authentication, encryption)\n",
    "- Reliability\n",
    "- Latency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Transport\n",
    "- Reliability and Integrity\n",
    "- Security\n",
    "- Latency\n",
    "- Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Storage\n",
    "- Flexibility and Ease of processing (keep data in native form or need to summarize it)\n",
    "- Schema or Schema Less design\n",
    "- High Availability/ Redundancy\n",
    "- Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Processing\n",
    "- Cleaning to remove inconsistency and bad data\n",
    "- Filtering - choosing relevant data\n",
    "- Enriching (data joints and denormalization)\n",
    "- Aggregating\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Servicing\n",
    "- Latency\n",
    "- Redundancy and High Availability\n",
    "- Skill Levels of consumers\n",
    "- Flexibility of schema\n",
    "- APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BigData Classified on below attributes\n",
    "- Volume (Resources req, scalable, maintaining latency)\n",
    "- Velocity (real-time event data handling, need for speed, handling lags)\n",
    "- Variety (Text, audio, video and images, more resources needed, Serving at low latency)\n",
    "- Variability (spikes in load, decoupling needed with buffering zones, maintaining latency)\n",
    "\n",
    "#### Pipeline\n",
    "- Functionality\n",
    "- Speed\n",
    "- Reliability\n",
    "- Security\n",
    "- Availability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark - Data processing engine  \n",
    "Apache Kafka - Data Aquisition and transport Layer  \n",
    "HDFS/MySQL - Storage  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Kafka Connect**\n",
    "- scalable distributed pipeline for moving data\n",
    "- Data Source -> Kafka -> Data Sink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark**\n",
    "- best tool for data engineering  \n",
    "\n",
    "Advantages:  \n",
    "1. Built as a compute engine\n",
    "2. Faster Data processing\n",
    "3. Massive Horizontal scalability\n",
    "4. Streaming Support\n",
    "5. Machine Learning Libraries (pyspark)\n",
    "6. Third Party integrations \n",
    "\n",
    "Features:  \n",
    "1. Spark Transformations - record level processing in distributed fashion, used in (1. Data Cleansing, 2. Data Validation, 3. Filtering, 4. Joining and Enriching Data, 5. Aggregation)\n",
    "2. Spark Actions - extract meaningful information from massive dataset (1. Metrics, 2. Aggregate data to provide summaries, 3. Moving data to external systems like File system and databases)\n",
    "3. Spark Broadcast variables and accumulators - minimize data across network and generalize system wide metrics(1. Lookup tables, 2. Shared variables, 3. Summary Metrics, 4. Data Consolidation)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
