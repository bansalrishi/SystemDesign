{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment of a machine learning model to production:\n",
    "The main focus here is not building a machine learning model or API, but to assess the ability to build\n",
    "the architecture and use open source tools for deployment of the model on cloud to production. You are\n",
    "welcome to choose any model API (Flask, Django, FastAPI, etc.) available publicly or already built by\n",
    "yourself. However, donâ€™t forget to use a web server gateway interface (WSGI) such as gunicorn in your\n",
    "API.\n",
    "Please make a proper explanation and documentation of your workflow. We consider it seriously.\n",
    "Follow the steps below for your assistance. We are open to other methods too.\n",
    "1. Build a big data pipeline for both stream processing and batch processing. You can refer to\n",
    "Lambda Architecture of Kafka.\n",
    "2. Use MongoDB for your data storage (sinking and sourcing)/Hadoop(HDFS).\n",
    "3. Build a CI/CD pipeline in 2 steps(Build,Tag) in Gitlab using the model API and tag the image in\n",
    "\":latest\" when you are on the master branch:\n",
    "i. Dockerize the API.\n",
    "ii. Create the file gitlab-ci.yml\n",
    "iii. Built-up images must be saved to the GitLab image container (under the branch name tag).\n",
    "iv. Start the Tag step only on the master branch and tag the image in \":latest\".\n",
    "v. The pipeline starts automatically when committing to any branch.\n",
    "vi. Orchestrate the docker container(s) using Kubernetes: Creation of Kubernetes clusters.\n",
    "Usage of nginx reverse proxy and async workers, task schedulers (Celery) are highly appreciated and\n",
    "bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "* Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "* Programming Language - Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image/spark.JPG\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Architecture \n",
    "* Build the data ingestion and reporting pipeline with Lambda Architecture using Spark\n",
    "* it provides code reusability between streaming and the batch layers, key configurations for the deployment and a few troubleshooting tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image/Lambda_spark.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lambda Architecture(LA) provides**\n",
    "* robust system that is fault-tolerant against hardware failures and human mistakes\n",
    "* system is linearly scalable (scaling out instead of scaling up)\n",
    "* there is no guarantee that data will come in one shot and there wont be any noise. Also, to provide protection against data that arrives late due to network or server instability\n",
    "* LA process the data twice to overcome above issues, once in realtime streaming and second time in a scheduled batch process\n",
    "* it writes to KairosDB idempotent in order to be able to reprocess the data and guarantee exactly once processing semantics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Spark Streaming**\n",
    "* provides unified api for streaming, batch and interactive analytics\n",
    "* its flexible , robust and scalable data processing engine\n",
    "* provides high throughput and good integration with data stores like Cassandra, Kafka, Hadoop, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "We use Spark Kafka Direct for our streaming that guarantees exactly once processing semantics with checkpointing enabled for recovery. Batch processing is built on top of core spark.The above mentioned metrics are calculated and stored in KairosDB, a time series DB. Also, the user session information is stored in HDFS as Hive External tables from the batch processing. The user state is maintained for 12 hours in streaming and in batch the user state is maintained as long as the user is active.\n",
    "RDDs in the streaming and the batch processing are processed as follows:\n",
    "Parsing and Validation: This is done in a series of map(), flatMap(), filter(), groupByKey(), reduceByKey() and leftOuterJoin() RDD transformations which gets reused across Streaming and Batch jobs.\n",
    "Data Enrichment and state maintenance: Data enrichment and state maintenance is done under updateStateByKey() in Streaming and mapPartitions() in case of batch processing with the business logic for merging the sessions and enriching the session with additional information getting reused across batch and streaming jobs. In streaming the state is maintained in memory and in the checkpoint as part of the functionality provided by updateStateByKey(). In batch processing, the state is maintained in Hive External Tables in HDFS. We also calculate rolling hour and 24 hour unique visitors using reduceByKeyAndWindow() with inverse function in the Streaming job.\n",
    "Metric Generation and Save to KairosDB: Metrics are generated using flatMap() and reduceByKey() after the Sessionization and data enrichment and are saved to KairosDB using a kairosDB client. The write to KairosDB happens in forEachPartition() action. The logic for the save gets reused in both the Streaming and batch processing.\n",
    "In the stream processing, the incoming beacons for each micro batch are grouped by sessionId and sorted by timestamp. Session state is updated by processing the sessions from the current micro batch and merging them with the session state from the previous micro batch. Metrics generated after sessionization are reduced and then stored to KairosDB. We store the offsets in Zookeeper in case we need to restart the job from the last successfully processed offsets. For accuracy, our transactions update the offsets in the same transaction when updating the results. The following code sample gives an overview of the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StreamingSpecificCode.txt**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//get or create streaming context\n",
    "  def main(args: Array[String]) {\n",
    "    try {\n",
    "      val context =   StreamingContext.getOrCreate(CHECKPOINT_DIRECTORY,\n",
    "          () => {\n",
    "            createFunc(args)\n",
    "          })\n",
    "      context.start()\n",
    "      context.awaitTermination()\n",
    "\n",
    "      sys.ShutdownHookThread{\n",
    "        LOGGER.info(\"Gracefully shutting down\")\n",
    "        context.stop(true,true)\n",
    "        LOGGER.info(\"Application stopped\")\n",
    "      }\n",
    "\n",
    "    }catch {\n",
    "      case ex: Exception => {\n",
    "        System.exit(1)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  def createFunc(args: Array[String]): StreamingContext = {\n",
    "  \n",
    "  //Start from the offsets stored in the zookeeper \n",
    " val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, (String, String)](ssc,     kafkaParams, topicAndPartitions, (mmd: MessageAndMetadata[String, String]) => (mmd.key, mmd.message))\n",
    "     \n",
    "  \n",
    "  //Getting the offset ranges of the input DStream\n",
    "  var offsetRanges = Array[OffsetRange]()\n",
    "\n",
    "  val kafkaStreamRdd = kafkaStream.transform { rdd =>\n",
    "    offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n",
    "    offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n",
    "    rdd\n",
    "  }\n",
    "  \n",
    "   //Do the filters etc and get the relevant beacons needed for Sessionization\n",
    "   val beaconMap = kafkaStreamRdd.transform(rdd => populateBeaconMap(rdd))\n",
    "   \n",
    "   //Group the sessions by sessionId and sort based on timestamp\n",
    "   val groupedAndSortedSessions = beaconMap.transform(rdd => populateGroupedAndSortedSessions(rdd))\n",
    "  \n",
    "   //Use updateStateByKey to maintain the state of the sessions\n",
    "   val statefulSessions = groupedAndSortedSessions.updateStateByKey(updateStatefulSessions)\n",
    "   \n",
    "   //Usage of checkpoint for stateful streaming\n",
    "   statefulSessions.checkpoint(Seconds(DSTREAM_CHECKPOINT_INTERVAL))\n",
    "   \n",
    "   //Generate metric values from stateful sessions\n",
    "   val metricValues = statefulSessions.transform(rdd => generateMetricValues(rdd))\n",
    "   \n",
    "   //Reduce the metrics generated\n",
    "   val reducedMeticValues = metricValues.transform(rdd => generateReducedMetrics(rdd))\n",
    "   \n",
    "   //Saving the metrics to KairosDB\n",
    "   reducedMeticValues.foreachRDD { rdd =>\n",
    "            //Create the broadcast variable of the curator framework to store the data in Zookeeper\n",
    "            val curatorSink = CuratorSinkBroadCast.initCuratorSink(rdd.sparkContext)\n",
    "            //Save the data to kairos DB          \n",
    "            saveToKairos(rdd, offsetRanges, curatorSink)\n",
    "            \n",
    "    }\n",
    "  \n",
    "  }\n",
    "  \n",
    " //Method used by updateStateByKey to produce Stateful Sessions\n",
    "  def updateStatefulSessions(\n",
    "                                 //(Sequential list of timestamp and Json)\n",
    "                                 currentSessionList: Seq[(List[(Long, String)])],\n",
    "                                 //(requestSession, mergedSession)\n",
    "                                 previousSessionRecord: Option[(StatefulSession, StatefulSession)]\n",
    "                                 ): Option[(StatefulSession, StatefulSession)] = {\n",
    "    //(requestSession,mergedSession)\n",
    "    var result: Option[(StatefulSession, StatefulSession)] = null\n",
    "    try {\n",
    "      //Invalidating the session if it's inactive for greater than 35 minutes\n",
    "      if (currentSessionList.size == 0) {\n",
    "          if ((System.currentTimeMillis() - previousSessionRecord.getRequestTime) > 2100000l) {\n",
    "              result = None\n",
    "          } \n",
    "        }\n",
    "      //Iterating through grouped and sorted sessions in the current batch and merge with the sessions from the previous batch\n",
    "      currentSessionList.foreach(currentSessionRecord => {\n",
    "        //Merge the session record from the current batch with the merged session from the previous batch  \n",
    "        result = processStatefulSessions(currentSessionRecord,previousSessionRecord)\n",
    "      })\n",
    "    }catch {\n",
    "      case ex: Exception => {   \n",
    "        throw new Exception(msg, ex)\n",
    "      }\n",
    "    }\n",
    "    result\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sample has the reusable functions that are called from the Stream processing and batch processing for filtering and populating the valid beacons, populating grouped and sorted sessions, processing Stateful Sessions , generating the metric values, reducing the metrics and storing the generated metrics to KairosDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CommonCodeBetweenBatchAndStreaming**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  //Do the filters etc and get the relevant beacons needed for Sessionization\n",
    "  def populateBeaconMap(rdd: RDD[(String, String)]): RDD[(String, (Long, String))] = {\n",
    "    rdd.flatMap[(String, (Long, String))] { case (key, inputBeacon) =>\n",
    "        val beaconData = constructBeaconData(inputBeacon)\n",
    "        if (beaconData !=null && beaconData.isValidBeacon()) {\n",
    "          Seq((beaconData.getSessionID, (beaconData.getTimestamp, inputBeacon)))\n",
    "        } else {\n",
    "          Seq((\"\", (0l, \"\")))\n",
    "        }\n",
    "    }\n",
    "  }\n",
    "  \n",
    " //Group the sessions by sessionId and sort based on timestamp \n",
    " def populateGroupedAndSortedSessions(rdd: RDD[(String, (Long, String))]): RDD[(String, List[(Long, String)])] = {\n",
    "    val groupedSessions = rdd.groupByKey()\n",
    "    val sortedSessions = groupedSessions.mapValues[(List[(Long, String)])](iter => iter.toList.sortBy(_._1))\n",
    "    sortedSessions\n",
    "  }\n",
    "  \n",
    "  //Process stateful sessions by creating new or merged sessions\n",
    "  def processStatefulSessions(currentSessionBeaconList: List[(Long, String)], previousBatchSessionRecord: StatefulSession): Option[(StatefulSession, StatefulSession)] = {\n",
    "        val beaconSessionization = new BeaconSessionization()\n",
    "        val sessionInfoArray = beaconSessionization.buildNewOrMergedSessions(currentSessionBeaconList, previousBatchSessionRecord)\n",
    "        val currentSession = sessionInfoArray(0).asInstanceOf[StatefulSession]\n",
    "        val mergedSession = if (sessionInfoArray(1) != null) sessionInfoArray(1).asInstanceOf[StatefulSession] else null\n",
    "        Some((currentSession, mergedSession))\n",
    "   }\n",
    "   \n",
    "   //Generate metric values from stateful sessions\n",
    "   def generateMetricValues(rdd: RDD[(String, (StatefulSession, StatefulSession))]): RDD[(Any)] = {\n",
    "    rdd.flatMap { case (sessionId, processStatefulSessions: (StatefulSession, StatefulSession)) =>\n",
    "          val metricGeneration = new MetricGeneration()\n",
    "          metricGeneration.getMetricValues(processStatefulSessions._1, processStatefulSessions._2).toArray.toTraversable\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  //Reduce the metrics generated  \n",
    "  def generateReducedMetrics(rdd: RDD[(String, (Long, Array[String]))]):\n",
    "    RDD[(String, (Long, Array[String]))] = {\n",
    "      rdd.reduceByKey((metricRecord1, metricRecord2) => {\n",
    "        (Math.max(metricRecord1._1, metricRecord2._1), (metricRecord1._2 ++ metricRecord2._2))\n",
    "     })\n",
    "   }\n",
    "  \n",
    "  //Saving the data to KairosDB  \n",
    "  def saveToKairos(recordsToSave: RDD[(String, (Long, Float))],offsetRanges:Array[OffsetRange], curatorSink: Broadcast[CuratorSink]) = {\n",
    "    recordsToSave.foreachPartition({ partitionOfMetricRecords =>\n",
    "        var metricList = new java.util.ArrayList[String]()\n",
    "        if (partitionOfMetricRecords != null) {\n",
    "               partitionOfMetricRecords.foreach(metricRecord => {\n",
    "               metricHourList.add(constructMetrics(metricRecord._1, metricRecord._2))\n",
    "            }\n",
    "          )\n",
    "        }\n",
    "        callKairosClientToSaveDataPoints(metricList)\n",
    "        //Committing the last processed offsets to Zookeeper\n",
    "        if (offsetRanges!=null && offsetRanges.length>0 && curatorSink!=null) {\n",
    "          val osr: OffsetRange = offsetRanges(TaskContext.get.partitionId)\n",
    "          val nodePath = \"/\" + consumerId + \"/offsets/\" + osr.topic + \"/\" + osr.partition\n",
    "          curatorSink.value.updateZookeeper(nodePath, osr.untilOffset)\n",
    "        }\n",
    "      })\n",
    "    }  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a code sample of the batch processing that does the Stateful Sessionization, Metric Generation and Save to KairosDB and the above functions that are called from Stream Processing are called from batch processing as well thus providing code reusability between the batch and stream processing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//Creating a new Spark Context and generating a new Spark Session\n",
    "\n",
    "val conf = new SparkConf().setAppName(\"BatchProcessing\")\n",
    "\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "val appSparkSession = SparkSession\n",
    "      .builder()\n",
    "      .enableHiveSupport()\n",
    "      .config(\"hive.scratch.dir.permission\", \"777\")\n",
    "      .config(\"spark.sql.warehouse.dir\", HIVE_WARE_HOUSE_DIR)\n",
    "      .config(conf)\n",
    "      .getOrCreate()\n",
    "      \n",
    " \n",
    " val job = Job.getInstance()\n",
    " val fs: FileSystem = FileSystem.get(job.getConfiguration)\n",
    " \n",
    " //Loading the beacon data from HDFS\n",
    " val beaconData = sc.sequenceFile(inputPaths, classOf[BytesWritable], classOf[Text], 800).\n",
    "          map { case (x, y) => (x.toString, y.toString) }\n",
    "          \n",
    " //Do the filters etc and get the relevant beacons needed for Sessionization\n",
    " val sessions = populateBeaconMap(beaconData)\n",
    "\n",
    " ///Group the sessions by sessionId and sort based on timestamp\n",
    " val groupedAndSortedSessions = populateGroupedAndSortedSessions(sessions) \n",
    " \n",
    " //Loading the previous job run records from HDFS stored in session table\n",
    " val prevSessionsRDD = loadMatchingSessionsFromThePreviousJobRunToMerge(fs, appSparkSession, sc, startTimeStamp.toLong*1000l)\n",
    "\n",
    " //CoGroup the incoming sessions with the matching sessions loaded from the previous job run\n",
    " val coGroupSessions = groupedAndSortedSessions.leftOuterJoin(prevSessionsRDD)\n",
    " \n",
    " //Enriching the session and merging the Session from the current batch with previous batch \n",
    " val statefulSessions = coGroupSessions.mapPartitions[(String, (StatefulSession, StatefulSession))]({ partition =>\n",
    "        val newOrMergedSessionsHashmap = collection.mutable.HashMap[String, (StatefulSession, StatefulSession)]()\n",
    "        //Iterating through grouped and sorted sessions in the current batch and merge with the sessions from the previous batch\n",
    "             partition.foreach(sessionRecordWithPreviousJobRun => {\n",
    "                var previousMergedSession: StatefulSession = fromJsonSession(sessionRecordWithPreviousJobRun._2._2.get)               \n",
    "                //Retrieve the current batch grouped and sorted session records\n",
    "                val currentSessionRecord = sessionRecordWithPreviousJobRun._2._1\n",
    "                //Merge the session record from the current batch with the merged session from the previous batch  \n",
    "                val newOrMergedSessions = processStatefulSessions(currentSessionRecord, previousMergedSession)\n",
    "                newOrMergedSessionsHashmap += sessionId ->(currentSession, mergedSession)\n",
    "            })\n",
    "            newOrMergedSessionsHashmap.toIterator         \n",
    "      }, true)\n",
    "      \n",
    "     //Generate an RDD with sessionId, mergedSession and hourlyPartitioner\n",
    "     val mergedSessionsRDDToBeSavedToHdfs = statefulSessions.map[(String, String, String)] { case (key, value) =>\n",
    "          (key, .toJson(value._2),\n",
    "            (TimeBucketUtil.getYear(value.getLastHit)\n",
    "              + \"-\" + TimeBucketUtil.getMonth(value.getLastHit)\n",
    "              + \"-\" + TimeBucketUtil.getDay(value.getLastHit)\n",
    "              + \"-\" + TimeBucketUtil.getHour(value.getLastHit)))\n",
    "        }  \n",
    "     //Create the dataframe from the RDD\n",
    "     val currentSessionsDFToWriteInHdfs = mergedSessionsRDDToBeSavedToHdfs.toDF(\"session_id\", \"session_json\",\n",
    "          \"hourly_partitioner\")\n",
    "     \n",
    "     //Register the dataframe with a temporary view\n",
    "     currentSessionsDFToWriteInHdfs.createOrReplaceTempView(\"sessionRecordsTemp\")  \n",
    "     \n",
    "     //Insert the session data to hive external tables which is used to maintain state\n",
    "     appSparkSession.sql(s\"  CREATE EXTERNAL TABLE IF NOT EXISTS ${SESSION_RECS_TABLE_NAME} (session_id STRING, session_json STRING) \" +\n",
    "          s\"PARTITIONED BY (hourly_partitioner STRING)    \" +\n",
    "          s\"stored as ORC LOCATION '${DATA_DIR}/${SESSION_RECS_TABLE_NAME}' \")\n",
    "\n",
    "     appSparkSession.sql(\n",
    "          s\"\"\" from sessionRecordsTemp ps  insert overwrite table ${SESSION_RECS_TABLE_NAME}\n",
    "            | partition(hourly_partitioner)\n",
    "            | select  ps.session_id, ps.session_json,\n",
    "            | ps.hourly_partitioner  \"\"\".stripMargin)\n",
    "            \n",
    "     //Generate metric values from stateful sessions\n",
    "     val metricValues = generateMetricValues(statefulSessions)\n",
    "     \n",
    "      //Reduce the metrics generated\n",
    "     val reducedMeticValues = generateReducedMetrics(metricValues)\n",
    "     \n",
    "     //Saving the metrics to KairosDB\n",
    "     saveToKairos(reducedMeticValues, null, null)\n",
    "     \n",
    "     //Method definition for loading the previous job run sessions\n",
    "     private def loadMatchingSessionsFromThePreviousJobRunToMerge(fs:FileSystem, appSparkSession:SparkSession, sc:   SparkContext, batchStartTime:Long):\n",
    "                                                               RDD[(String, String)] = {\n",
    "        val prevSessionQryResultsForId = appSparkSession.sql(\"SELECT ps.session_id,  ps.session_json  \" +\n",
    "          s\" from ${SESSION_RECS_TABLE_NAME} ps  \" +\n",
    "          \" where  ps.hourly_partitioner = '\"+formatYYYY_MM_DD_HH(batchStartTime - 3600l) +\"'\")\n",
    "\n",
    "        val prevSessionsRDD = prevSessionQryResultsForId.rdd.map[(String, String)](rowRec =>\n",
    "          (rowRec.getAs[String](\"session_id\"), rowRec.getAs[String](\"session_json\")))\n",
    "\n",
    "        prevSessionsRDD\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Configuration and Deployment\n",
    "We use a standalone cluster for Spark Streaming job deployment. For HA/DRwe run the Streaming Job in 2 different regions. The Streaming job from only one region writes to Kairos cluster at any point of time and the job in the second region keeps running by maintaining the state with no writes to kairos. If there is any issue with the job in the first region or if we need to do any maintenance to the cluster or upgrade the code or if there are any issues with the Kafka topic in one region, we switch the writes to Kairos from the job running in the second region. This gives us higher availability of the Streaming Job with session state being maintained and the data written to kairos is continuous.\n",
    "Our streaming job runs with a micro batch of 60 seconds. The correct micro batch size is chosen depending on the processing times. We started with a batch window size of 10 seconds and observed processing times and increased it incrementally until the processing times are completed within the micro batch time period and there is no scheduling delay or itâ€™s only increasing occasionally and recovers quickly. The ideal number of executors/cores is dependent on the application by considering various factors like number of peak events per second, maximum allowed lag and the buffering capabilities of the streaming source which can be arrived at by testing in a pre-production environment. The right amount of parallelism is between 2â€“3 times the number of cores and needs to be arrived at iteratively by testing the job with various configurations. Another key consideration is to set spark.memory.fraction and spark.memory.storageFraction to the right values. Based on various tests, it is observed that spark.locality.wait=0s is good for the job performance. We use kryo serialization in our Spark Jobs for better performance. When upgrading the application code, the application needs to be shutdown gracefully with no further records to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Monitoring and Automatic Job Restarts for Streaming\n",
    "We use a python script that runs every 5 minutes to monitor the streaming job to see if its up and running. We monitor if there is any lag, total delay, number of failed jobs vs number of successful jobs and take appropriate steps if it exceeds a certain threshold. We query for these metrics from Spark UI end point http://streamingUIURL:4040/metrics/json and http://streamingUIURL:4040/api/v1/applications/<appId>/jobs. If the job is down for some reason or the number of failure jobs are greater than the number of successful jobs for the past 5 minutes or if any of the task is stuck due to any infrastructure issues then the job gets restarted automatically and we receive an alert email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting the streaming job\n",
    "We use Spark streaming UI to troubleshoot any issues with the job. Following is a screenshot of the Streaming UI. Ideally the scheduling delay should be 0 or even if it increases, it should recover quickly within acceptable time periods. The processing time should be less than or equal to the microbatch time period. The Kafka Direct stream consumption parallelism is equal to the number of Kafka partitions and the number of partitions need to be adjusted if there is not enough parallelism for the reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of increased processing times, individual batches can be drilled down to see which stage is taking time as shown below. Shuffle reads/writes should be as less as possible and there should not be any data skew. If the shuffles are taking a lot of time, then one way of fixing that is by increasing the cores and the parallelism.\n",
    "DAG visualization as shown below is used to determine if there are any pipelining operations that are separated by shuffles. Performance is good if all the operations go through a single pipeline without any shuffles. It also gives an insight if any of the RDDs are cached. The cached RDDs are denoted by a green highlight. Caching this RDD means future computations on this RDD can be accessed from memory thereby improving the performance.\n",
    "The storage section of streaming UI as shown below provides an insight to see if the cached RDDs are regularly cleared or not. Ideally cached RDDs should be cleared at regular intervals to avoid any performance bottlenecks.\n",
    "The executors tab of the Web UI as shown below also provides information about the number of active tasks, utilization of the cores, task GC time, shuffle reads/writes. If there is a huge difference between active tasks vs cores allocated then there is under utilization of the cluster and the core allocation need to be adjusted accordingly. Thread dump is used to drill down on possible performance issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Configuration and Deployment\n",
    "We use Mesos as our cluster manager for batch jobs and use HDFS for state maintenance and intermediate file storage. The batch processing job runs every 30 minutes and processes any new data available in HDFS. The data is stored in Hive external tables. We configured the executor memory depending on the amount of data that is getting processed in each batch. We also use shuffle operations like reduceByKey, groupByKey, repartition, coalesce, cogroup, join etc as part of the batch job, the configs like spark.executor.memory, spark.cores.max, spark.default.parallelism were adjusted according to the job requirement iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting the batch job\n",
    "A few slow local tasks could cause a huge performance impact when one stage needs to finish before the next one can start. The detailed stage/tasks link in the Spark UI can be used to identify slow running nodes, nodes with resources problems, skew in data partitioning which can be identified by looking at the input size/records, or a small number of tasks taking significantly longer time to execute than the others. Drilling down into slower running tasks we can determine if the slowness is in writing data, reading data, or computation. If the processing is slow, it can be due to not enough resources and we would need to focus on how much memory and cpu has been allocated for the executers and also the total number of cores allocated for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "Conclusion\n",
    "\n",
    "To recap, here are some points covered in this article\n",
    "\n",
    "    Building the data pipeline for A/B testing with the lambda architecture using Spark helped us to have quick view of the data/metrics that get generated with a streaming job, and a reliable view from a scheduled batch process.\n",
    "    Using Spark/Spark Streaming helped us to write the business logic functions once, and then reuse the code in a batch ETL process as well as a streaming process which helped us lower the risk for errors resulting from duplicate code bases and also helped with the developer productivity as it provides a unified api for streaming, batch and interactive analytics.\n",
    "    We focused on having a stable stream/batch processing application first before focusing on the throughput. The performance of the applications were improved by tuning Sparkâ€™s serialization, memory parameters, increasing the number of cores and parallelism iteratively.\n",
    "    Spark Streaming/Batch UI provides very good information on the performance bottlenecks like shuffles, data skew, slow running tasks due to resource issues, task GC time, shuffle reads/writes, slow running stages, storage and other information to help troubleshoot the jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "1. https://medium.com/walmartglobaltech/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
